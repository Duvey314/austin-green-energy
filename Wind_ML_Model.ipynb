{
 "cells": [
  {
   "source": [
    "# Wind Machine Learning Model\n",
    "This notebook creates a machine learning model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Imports\n",
    "from path import Path\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Database Connection\n",
    "import config\n",
    "import pymongo\n",
    "\n",
    "# datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, balanced_accuracy_score\n",
    "\n",
    "# don't show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "source": [
    "# Import Data from Database"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mongodb connected\n"
     ]
    }
   ],
   "source": [
    "# set string variables\n",
    "DEFAULT_DATABASE = 'wind_solar_data' \n",
    "USERNAME = config.USERNAME\n",
    "PASSWORD = config.PASSWORD\n",
    "\n",
    "#create connection to database\n",
    "client = pymongo.MongoClient(f\"mongodb+srv://{USERNAME}:{PASSWORD}@austin-green-energy.pwzpm.mongodb.net/{DEFAULT_DATABASE}?retryWrites=true&w=majority\")\n",
    "try:\n",
    "    client.server_info()\n",
    "    print(\"Mongodb connected\")\n",
    "except:\n",
    "    print(\"The Mongodb failed to connect. Check username/password in connection string.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                           _id                 time  WindSpeed_mph  \\\n",
       "0     5f946bc9c64c67a0641fc6f8  2019-01-01 01:00:00           13.0   \n",
       "1     5f946bc9c64c67a0641fc6f9  2019-01-01 02:00:00           14.0   \n",
       "2     5f946bc9c64c67a0641fc6fa  2019-01-01 03:00:00           15.0   \n",
       "3     5f946bc9c64c67a0641fc6fb  2019-01-01 04:00:00           14.0   \n",
       "4     5f946bc9c64c67a0641fc6fc  2019-01-01 05:00:00           14.0   \n",
       "...                        ...                  ...            ...   \n",
       "8755  5f946bc9c64c67a0641fe92b  2019-12-31 19:00:00            6.0   \n",
       "8756  5f946bc9c64c67a0641fe92c  2019-12-31 20:00:00            6.0   \n",
       "8757  5f946bc9c64c67a0641fe92d  2019-12-31 21:00:00            7.0   \n",
       "8758  5f946bc9c64c67a0641fe92e  2019-12-31 22:00:00            7.0   \n",
       "8759  5f946bc9c64c67a0641fe92f  2019-12-31 23:00:00            7.0   \n",
       "\n",
       "      WindDirection_degrees WindDirection_compass  WindGust_mph         MWH  \n",
       "0                      89.0                     E          23.0  110.487950  \n",
       "1                      53.0                    NE          23.0   72.020225  \n",
       "2                      17.0                   NNE          22.0   67.639475  \n",
       "3                      18.0                   NNE          21.0   63.718900  \n",
       "4                      19.0                   NNE          20.0   61.264250  \n",
       "...                     ...                   ...           ...         ...  \n",
       "8755                  175.0                     S          12.0   15.506725  \n",
       "8756                  176.0                     S          13.0   32.191125  \n",
       "8757                  176.0                     S          14.0   40.677250  \n",
       "8758                  176.0                     S          15.0   45.826475  \n",
       "8759                  175.0                     S          16.0   59.101325  \n",
       "\n",
       "[8760 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_id</th>\n      <th>time</th>\n      <th>WindSpeed_mph</th>\n      <th>WindDirection_degrees</th>\n      <th>WindDirection_compass</th>\n      <th>WindGust_mph</th>\n      <th>MWH</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5f946bc9c64c67a0641fc6f8</td>\n      <td>2019-01-01 01:00:00</td>\n      <td>13.0</td>\n      <td>89.0</td>\n      <td>E</td>\n      <td>23.0</td>\n      <td>110.487950</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5f946bc9c64c67a0641fc6f9</td>\n      <td>2019-01-01 02:00:00</td>\n      <td>14.0</td>\n      <td>53.0</td>\n      <td>NE</td>\n      <td>23.0</td>\n      <td>72.020225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5f946bc9c64c67a0641fc6fa</td>\n      <td>2019-01-01 03:00:00</td>\n      <td>15.0</td>\n      <td>17.0</td>\n      <td>NNE</td>\n      <td>22.0</td>\n      <td>67.639475</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5f946bc9c64c67a0641fc6fb</td>\n      <td>2019-01-01 04:00:00</td>\n      <td>14.0</td>\n      <td>18.0</td>\n      <td>NNE</td>\n      <td>21.0</td>\n      <td>63.718900</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5f946bc9c64c67a0641fc6fc</td>\n      <td>2019-01-01 05:00:00</td>\n      <td>14.0</td>\n      <td>19.0</td>\n      <td>NNE</td>\n      <td>20.0</td>\n      <td>61.264250</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8755</th>\n      <td>5f946bc9c64c67a0641fe92b</td>\n      <td>2019-12-31 19:00:00</td>\n      <td>6.0</td>\n      <td>175.0</td>\n      <td>S</td>\n      <td>12.0</td>\n      <td>15.506725</td>\n    </tr>\n    <tr>\n      <th>8756</th>\n      <td>5f946bc9c64c67a0641fe92c</td>\n      <td>2019-12-31 20:00:00</td>\n      <td>6.0</td>\n      <td>176.0</td>\n      <td>S</td>\n      <td>13.0</td>\n      <td>32.191125</td>\n    </tr>\n    <tr>\n      <th>8757</th>\n      <td>5f946bc9c64c67a0641fe92d</td>\n      <td>2019-12-31 21:00:00</td>\n      <td>7.0</td>\n      <td>176.0</td>\n      <td>S</td>\n      <td>14.0</td>\n      <td>40.677250</td>\n    </tr>\n    <tr>\n      <th>8758</th>\n      <td>5f946bc9c64c67a0641fe92e</td>\n      <td>2019-12-31 22:00:00</td>\n      <td>7.0</td>\n      <td>176.0</td>\n      <td>S</td>\n      <td>15.0</td>\n      <td>45.826475</td>\n    </tr>\n    <tr>\n      <th>8759</th>\n      <td>5f946bc9c64c67a0641fe92f</td>\n      <td>2019-12-31 23:00:00</td>\n      <td>7.0</td>\n      <td>175.0</td>\n      <td>S</td>\n      <td>16.0</td>\n      <td>59.101325</td>\n    </tr>\n  </tbody>\n</table>\n<p>8760 rows Ã— 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# select database\n",
    "db = client.get_database('wind_solar_data')\n",
    "# select collection\n",
    "collection = db.wind_data\n",
    "\n",
    "# pull collection into dataframe\n",
    "wind_df = pd.DataFrame(list(collection.find()))\n",
    "wind_df"
   ]
  },
  {
   "source": [
    "### Drop Columns\n",
    "The first cleaning is to drop the columns we dont't need. We'll be dropping the _id column because this is an artifact of the Mongodb storage and isn't a feature of the dataset. The time column will be dropped because there is not a linear relationship between time and wind power. The winddirection compas is dropped because this data is less granular than the winddirection degrees."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop uneeded columns\n",
    "wind_clean_df = wind_df.drop(['_id', \"WindDirection_compass\"], axis=1)"
   ]
  },
  {
   "source": [
    "### Type Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "time                     0\n",
       "WindSpeed_mph            0\n",
       "WindDirection_degrees    0\n",
       "WindGust_mph             0\n",
       "MWH                      0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "wind_clean_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any NaN values\n",
    "wind_clean_df = wind_clean_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "time                     datetime64[ns]\n",
       "WindSpeed_mph                     int32\n",
       "WindDirection_degrees             int32\n",
       "WindGust_mph                      int32\n",
       "MWH                               int32\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "wind_clean_df[\"WindSpeed_mph\"] = wind_clean_df[\"WindSpeed_mph\"].round(0).astype(int)\n",
    "wind_clean_df[\"WindDirection_degrees\"] = wind_clean_df[\"WindDirection_degrees\"].round(0).astype(int)\n",
    "wind_clean_df[\"WindGust_mph\"] = wind_clean_df[\"WindGust_mph\"].round(0).astype(int)\n",
    "wind_clean_df['time'] = pd.to_datetime(wind_clean_df['time'])\n",
    "wind_clean_df[\"MWH\"] = wind_clean_df[\"MWH\"].round(0).astype(int)\n",
    "wind_clean_df.dtypes"
   ]
  },
  {
   "source": [
    "# ML Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Date time not supported in linear Regression."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Split Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features set.\n",
    "#wind_clean_df = wind_clean_df.reset_index()\n",
    "X = wind_clean_df.drop([\"MWH\",'time'], axis=1)\n",
    "y = wind_clean_df[\"MWH\"].ravel()\n",
    "\n",
    "#split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a StandardScaler instance.\n",
    "scaler = StandardScaler()\n",
    "# Fitting the Standard Scaler with the training data.\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling the data.\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Train\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train_scaled,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 250.91702586,  708.69483708,  754.48646983, ...,  577.67519832,\n",
       "       1124.06354202, 1293.8023135 ])"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "# test\n",
    "y_pred = regr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "38.07819369662388 %\nR^2 Value:0.38078193696623874\n"
     ]
    }
   ],
   "source": [
    "accuracy = regr.score(X_test_scaled,y_test)\n",
    "print(accuracy*100,'%')\n",
    "print(f\"R^2 Value:{regr.score(X_test_scaled,y_test)}\")\n"
   ]
  },
  {
   "source": [
    "## Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "6568/6568 [==============================] - 0s 28us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 2/50\n",
      "6568/6568 [==============================] - 0s 20us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 3/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 4/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 5/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 6/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 7/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 8/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 9/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 10/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 11/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 12/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 13/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 14/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 15/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 16/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 17/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 18/50\n",
      "6568/6568 [==============================] - 0s 20us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 19/50\n",
      "6568/6568 [==============================] - 0s 20us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 20/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 21/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 22/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 23/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 24/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 25/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 26/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 27/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 28/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 29/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 30/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 31/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 32/50\n",
      "6568/6568 [==============================] - 0s 20us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 33/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 34/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 35/50\n",
      "6568/6568 [==============================] - 0s 22us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 36/50\n",
      "6568/6568 [==============================] - 0s 20us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 37/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 38/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 39/50\n",
      "6568/6568 [==============================] - 0s 22us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 40/50\n",
      "6568/6568 [==============================] - 0s 18us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 41/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 42/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 43/50\n",
      "6568/6568 [==============================] - 0s 21us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 44/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 45/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 46/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 47/50\n",
      "6568/6568 [==============================] - 0s 21us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 48/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 49/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "Epoch 50/50\n",
      "6568/6568 [==============================] - 0s 19us/sample - loss: nan - acc: 0.0606\n",
      "2190/2190 - 0s - loss: nan - acc: 0.0689\n",
      "Loss: nan, Accuracy: 0.06894977390766144\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Define the model - deep neural net\n",
    "number_input_features = 3\n",
    "hidden_nodes_layer1 =  number_input_features*3\n",
    "hidden_nodes_layer2 =  number_input_features*3\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"linear\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"linear\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn.compile(optimizer=\"sgd\", loss=tf.keras.losses.MeanSquaredError(), metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'KerasRegressor' object has no attribute 'evaluate'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-e62965086dc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#r2_score(y_test, prediction)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mmodel_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Loss: {model_loss}, Accuracy: {model_accuracy}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KerasRegressor' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "# Example from https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "seed = 1\n",
    "\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=3, activation='linear'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "# estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=100, verbose=False)\n",
    "# kfold = KFold(n_splits=10, random_state=seed)\n",
    "# results = cross_val_score(estimator, X, y, cv=kfold)\n",
    "# print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "\n",
    "# estimator.fit(X, y)\n",
    "# prediction = estimator.predict(X)\n",
    "# accuracy_score(y, prediction)\n",
    "\n",
    "estimator = KerasRegressor(build_fn=baseline_model)\n",
    "estimator.fit(X_train_scaled, y_train, nb_epoch=100, batch_size=100, verbose=False, shuffle=False)\n",
    "prediction = estimator.predict(X_test_scaled)\n",
    "r2_score(y_test, prediction)\n",
    "\n",
    "# model = baseline_model()\n",
    "# model.fit(X, y, nb_epoch=100, batch_size=100, verbose=False, shuffle=False)\n",
    "# prediction = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.09043848964677223"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "# https://nbviewer.jupyter.org/github/srnghn/ml_example_notebooks/blob/master/Predicting%20Wine%20Types%20with%20Neural%20Networks.ipynb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "accuracy_score(y_train, model.predict(X_train_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([28, 49, 15, ..., 86, 28, 11])"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 14,   1,   0, ..., 115,  24,  14])"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "model.predict(X_train_scaled)"
   ]
  },
  {
   "source": [
    "## Random Forrest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the training data with the RandomOversampler\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "brfc = BalancedRandomForestClassifier(n_estimators=100, random_state=1)\n",
    "brfc.fit(X_train, y_train)\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated the balanced accuracy score\n",
    "y_pred = brfc.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the imbalanced classification report\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the features sorted in descending order by feature importance\n",
    "importances = brfc.feature_importances_\n",
    "sorted(zip(brfc.feature_importances_, X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('PythonData': conda)",
   "display_name": "Python 3.7.7 64-bit ('PythonData': conda)",
   "metadata": {
    "interpreter": {
     "hash": "afc7c4a812fa55c0db985e1c504d2cd5747faf27fd01c3486b2fd8146566cace"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}